{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece7c4f4-99c8-4238-84d3-66fca28ec57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2eb1d51-0972-42a1-a848-e3514c3af1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 15:46:20,705\tINFO worker.py:1771 -- Connecting to existing Ray cluster at address: 10.244.15.75:6379...\n",
      "2025-10-15 15:46:20,721\tINFO worker.py:1942 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.15.75:8265 \u001b[39m\u001b[22m\n",
      "[2025-10-15 15:46:20,725 I 4209 4209] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f42607a9da42f5a179b79b9460b633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.12</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.49.0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://10.244.15.75:8265\" target=\"_blank\">http://10.244.15.75:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='10.244.15.75:8265', python_version='3.10.12', ray_version='2.49.0', ray_commit='66438d8bd27f8c604ee5a0cd2cfc5649053285ed')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=1374, ip=10.244.16.11)\u001b[0m INFO 2025-10-15 15:46:26,816 proxy 10.244.16.11 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:26,917 controller 1497 -- Removing 1 replica from Deployment(name='LLMServer:Qwen--Qwen3-14B', app='default').\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:26,917 controller 1497 -- Removing 2 replicas from Deployment(name='LLMRouter', app='default').\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:26,981 controller 1497 -- Draining proxy on node '1ef2025d3c4568da122e009fb7efeeb0951436f98a1669f1614d5349'.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:29,093 controller 1497 -- Replica(id='f3f4fiap', deployment='LLMServer:Qwen--Qwen3-14B', app='default') is stopped.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:29,094 controller 1497 -- Replica(id='og0ap10u', deployment='LLMRouter', app='default') is stopped.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:29,095 controller 1497 -- Replica(id='hscfbwev', deployment='LLMRouter', app='default') is stopped.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:46:58,304 controller 1497 -- Removing drained proxy on node '1ef2025d3c4568da122e009fb7efeeb0951436f98a1669f1614d5349'.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:47:29,651 controller 1497 -- Deploying new version of Deployment(name='LLMServer:Qwen--Qwen3-14B', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:47:29,652 controller 1497 -- Deploying new version of Deployment(name='LLMRouter', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=1499)\u001b[0m INFO 2025-10-15 15:47:29,694 proxy 10.244.15.75 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:47:29,794 controller 1497 -- Adding 1 replica to Deployment(name='LLMServer:Qwen--Qwen3-14B', app='default').\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m INFO 2025-10-15 15:47:29,796 controller 1497 -- Adding 2 replicas to Deployment(name='LLMRouter', app='default').\n",
      "\u001b[33m(raylet, ip=10.244.15.139)\u001b[0m [2025-10-15 15:47:33,596 I 1919 1919] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=1588, ip=10.244.16.11)\u001b[0m INFO 10-15 15:47:35 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=1587, ip=10.244.16.11)\u001b[0m INFO 2025-10-15 15:47:37,199 proxy 10.244.16.11 -- Proxy starting on node 1ef2025d3c4568da122e009fb7efeeb0951436f98a1669f1614d5349 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=1587, ip=10.244.16.11)\u001b[0m INFO 2025-10-15 15:47:37,371 proxy 10.244.16.11 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=1587, ip=10.244.16.11)\u001b[0m INFO 2025-10-15 15:47:37,435 proxy 10.244.16.11 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7f859c26a560>.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:47:40,061 default_LLMServer:Qwen--Qwen3-14B kehryfqx -- Running tasks to download model files on worker nodes\n",
      "\u001b[33m(raylet, ip=10.244.15.139)\u001b[0m [2025-10-15 15:47:43,734 I 1995 1995] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[36m(ProxyActor pid=641, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:47:37,413 proxy 10.244.15.139 -- Proxy starting on node 4f596807df2906b1e05612ebd20548a44a28f68e11cc1354b5bd74af (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=641, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:47:37,565 proxy 10.244.15.139 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=641, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:47:37,619 proxy 10.244.15.139 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7fb204b7a710>.\n",
      "\u001b[36m(download_model_files pid=1995, ip=10.244.15.139)\u001b[0m No cloud storage mirror configured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(download_model_files pid=1995, ip=10.244.15.139)\u001b[0m INFO 10-15 15:47:47 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_get_vllm_engine_config pid=1995, ip=10.244.15.139)\u001b[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[33m(raylet, ip=10.244.16.11)\u001b[0m [2025-10-15 15:47:43,768 I 2584 2584] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_get_vllm_engine_config pid=1995, ip=10.244.15.139)\u001b[0m INFO 10-15 15:47:50 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n",
      "\u001b[36m(_get_vllm_engine_config pid=1995, ip=10.244.15.139)\u001b[0m INFO 10-15 15:47:50 [model.py:1510] Using max model len 8192\n",
      "\u001b[36m(_get_vllm_engine_config pid=1995, ip=10.244.15.139)\u001b[0m INFO 10-15 15:47:50 [arg_utils.py:1215] Using ray runtime env: {'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_get_vllm_engine_config pid=1995, ip=10.244.15.139)\u001b[0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[36m(download_model_files pid=2584, ip=10.244.16.11)\u001b[0m No cloud storage mirror configured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_get_vllm_engine_config pid=1995, ip=10.244.15.139)\u001b[0m INFO 10-15 15:47:51 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m You are using a model of type qwen3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m You are using a model of type qwen3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:47:51,560 default_LLMServer:Qwen--Qwen3-14B kehryfqx -- Clearing the current platform cache ...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:47:51,562 default_LLMServer:Qwen--Qwen3-14B kehryfqx -- Using executor class: <class 'vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m WARNING 10-15 15:47:52 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:47:58 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=1497)\u001b[0m WARNING 2025-10-15 15:47:59,837 controller 1497 -- Deployment 'LLMServer:Qwen--Qwen3-14B' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m WARNING 2025-10-15 15:47:59,838 controller 1497 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m 2025-10-15 15:48:00,009\tINFO worker.py:1630 -- Using address rayheadservice.ray-app-core-schema:6379 set in the environment variable RAY_ADDRESS\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m 2025-10-15 15:48:00,012\tINFO worker.py:1771 -- Connecting to existing Ray cluster at address: rayheadservice.ray-app-core-schema:6379...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:47:59 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:48:00 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m 2025-10-15 15:48:00,177\tINFO worker.py:1942 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.15.75:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m [2025-10-15 15:48:00,233 I 2047 2047] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:48:00 [ray_utils.py:324] Using the existing placement group\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:48:00 [ray_distributed_executor.py:171] use_ray_spmd_worker: True\n",
      "\u001b[36m(pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:48:09 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:48:09 [ray_env.py:65] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:48:09 [ray_env.py:68] If certain env vars should NOT be copied, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m WARNING 10-15 15:48:09 [__init__.py:947] Overwriting environment variable LD_LIBRARY_PATH from '/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:11 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:11 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Bootstrap: Using eth0:10.244.15.139<0>\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO cudaDriverVersion 12080\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO NCCL version 2.27.3+cuda12.9\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. \n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO NET/Socket : Using [0]eth0:10.244.15.139<0>\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Initialized NET plugin Socket\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Assigned NET plugin Socket to comm\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Using network Socket\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO ncclCommInitRank comm 0x55c672499230 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x2816b91729dc9391 - Init START\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO RAS client listening socket at ::1<28028>\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Bootstrap timings total 0.003008 (create 0.000119, send 0.000223, recv 0.000749, ring 0.000245, delay 0.000001)\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO comm 0x55c672499230 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Channel 00/02 : 0 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Channel 01/02 : 0 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2182 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2183 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO ncclCommInitRank comm 0x55c672499230 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x2816b91729dc9391 - Init COMPLETE\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 2 total 0.13 (kernels 0.12, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2712 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2711 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2711 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2711 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2711 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2711 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:12 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m WARNING 10-15 15:48:12 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:12 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[36m(pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:07 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:12 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "\u001b[33m(raylet, ip=10.244.16.11)\u001b[0m [2025-10-15 15:48:04,080 I 2629 2629] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:00<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:18 [default_loader.py:267] Loading weights took 4.82 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m WARNING 10-15 15:48:09 [__init__.py:947] Overwriting environment variable LD_LIBRARY_PATH from '/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:11 [__init__.py:1384] Found nccl from library libnccl.so.2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:11 [pynccl.py:103] vLLM is using nccl==2.27.3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Bootstrap: Using eth0:10.244.16.11<0>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO cudaDriverVersion 12080\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NCCL version 2.27.3+cuda12.9\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Failed to open libibverbs.so[.1]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NET/Socket : Using [0]eth0:10.244.16.11<0>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Initialized NET plugin Socket\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Assigned NET plugin Socket to comm\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Using network Socket\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO ncclCommInitRank comm 0x55cf45bcfab0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x2816b91729dc9391 - Init START\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO RAS client listening socket at ::1<28028>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Bootstrap timings total 0.007588 (create 0.000126, send 0.000745, recv 0.004833, ring 0.000256, delay 0.000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO comm 0x55cf45bcfab0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Channel 01/02 : 0 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO P2P Chunksize set to 131072\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2709 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2710 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO ncclCommInitRank comm 0x55cf45bcfab0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x2816b91729dc9391 - Init COMPLETE\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.13 (kernels 0.11, alloc 0.00, bootstrap 0.01, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2185 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2184 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/Socket/0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2184 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/Socket/0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2184 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m WARNING 10-15 15:48:12 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:12 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-14B...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [gpu_model_runner.py:2634] Loading model from scratch...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(pid=2096)\u001b[0m INFO 10-15 15:48:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [cuda.py:366] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2184 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/Socket/0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2184 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/Socket/0\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:18 [gpu_model_runner.py:2653] Model loading took 13.7641 GiB and 5.530341 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=1497)\u001b[0m WARNING 2025-10-15 15:48:29,905 controller 1497 -- Deployment 'LLMServer:Qwen--Qwen3-14B' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m WARNING 2025-10-15 15:48:29,906 controller 1497 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[33m(raylet, ip=10.244.16.11)\u001b[0m [2025-10-15 15:48:04,080 I 2629 2629] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.66it/s]\u001b[32m [repeated 13x across cluster]\u001b[0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:51 [default_loader.py:267] Loading weights took 37.56 seconds\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m WARNING 10-15 15:48:09 [__init__.py:947] Overwriting environment variable LD_LIBRARY_PATH from '/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:11 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:11 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Bootstrap: Using eth0:10.244.16.11<0>\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO cudaDriverVersion 12080\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NCCL version 2.27.3+cuda12.9\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. \n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO NET/Socket : Using [0]eth0:10.244.16.11<0>\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Initialized NET plugin Socket\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Assigned NET plugin Socket to comm\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Using network Socket\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO ncclCommInitRank comm 0x55cf45bcfab0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x2816b91729dc9391 - Init START\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO RAS client listening socket at ::1<28028>\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Bootstrap timings total 0.007588 (create 0.000126, send 0.000745, recv 0.004833, ring 0.000256, delay 0.000000)\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO comm 0x55cf45bcfab0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2709 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2710 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO ncclCommInitRank comm 0x55cf45bcfab0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x2816b91729dc9391 - Init COMPLETE\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2629 [0] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.13 (kernels 0.11, alloc 0.00, bootstrap 0.01, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2185 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m statefulset-1:2096:2184 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m WARNING 10-15 15:48:12 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m INFO 10-15 15:48:12 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:12 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m INFO 10-15 15:48:18 [gpu_model_runner.py:2653] Model loading took 13.7641 GiB and 5.530341 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:51 [gpu_model_runner.py:2653] Model loading took 13.7641 GiB and 38.301397 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:56 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/7e3bf218a9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:56 [backends.py:559] Dynamo bytecode transform time: 4.13 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:51 [default_loader.py:267] Loading weights took 37.56 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:57 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.388 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:51 [gpu_model_runner.py:2653] Model loading took 13.7641 GiB and 38.301397 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:58 [monitor.py:34] torch.compile takes 4.13 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=1497)\u001b[0m WARNING 2025-10-15 15:48:59,932 controller 1497 -- Deployment 'LLMServer:Qwen--Qwen3-14B' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m WARNING 2025-10-15 15:48:59,932 controller 1497 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=1497)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:00 [gpu_worker.py:298] Available KV cache memory: 4.94 GiB\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:49:01 [kv_cache_utils.py:1087] GPU KV cache size: 64,784 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:49:01 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 7.91x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:49:01 [kv_cache_utils.py:1087] GPU KV cache size: 62,736 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:49:01 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 7.66x\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:56 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/0d7c7404f8/rank_0_0/backbone for vLLM's torch.compile\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:56 [backends.py:559] Dynamo bytecode transform time: 4.10 s\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|         | 2/19 [00:00<00:00, 18.40it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|| 19/19 [00:00<00:00, 20.97it/s]\n",
      "Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Capturing CUDA graphs (decode, FULL):  27%|       | 3/11 [00:00<00:00, 20.52it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|| 11/11 [00:00<00:00, 22.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:49:03 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.72 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:57 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.413 s\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:03 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.20 GiB\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:49:04 [core.py:149] Batch queue is enabled with size 2\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m INFO 10-15 15:48:58 [monitor.py:34] torch.compile takes 4.10 s in total\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:04 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 3921\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:04 [api_server.py:1634] Supported_tasks: ['generate']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m WARNING 10-15 15:49:04 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:04 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:04 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:49:04 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:49:04,857 default_LLMServer:Qwen--Qwen3-14B kehryfqx -- Started vLLM engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:49:04,995 default_LLMServer:Qwen--Qwen3-14B kehryfqx 6f675e56-295e-4a16-ab1a-91f6c2786f53 -- CALL llm_config OK 3.1ms\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:49:04,995 default_LLMServer:Qwen--Qwen3-14B kehryfqx 815f58c8-4a78-4927-be40-7851ade502b7 -- CALL llm_config OK 2.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:51:13 [chat_utils.py:560] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 10-15 15:51:13 [async_llm.py:316] Added request chatcmpl-02468d42-12d2-4d15-a6aa-886fa13abe5d.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:51:13 [ray_distributed_executor.py:552] RAY_CGRAPH_get_timeout is set to 300\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:51:13 [ray_distributed_executor.py:554] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:51:13 [ray_distributed_executor.py:556] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m INFO 10-15 15:51:13 [ray_distributed_executor.py:621] Using RayPPCommunicator (which wraps vLLM _PP GroupCoordinator) for Ray Compiled Graph communication.\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:49:01 [gpu_worker.py:298] Available KV cache memory: 4.79 GiB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m INFO 10-15 15:48:56 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/0d7c7404f8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m INFO 10-15 15:48:56 [backends.py:559] Dynamo bytecode transform time: 4.10 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:48:57 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.413 s\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m INFO 10-15 15:49:03 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.20 GiB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2096 [0] NCCL INFO Comm config Blocking set to 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Assigned NET plugin Socket to comm\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Using network Socket\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO ncclCommInitRankConfig comm 0x55c68e0769f0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0xbe71890bb1555e6c - Init START\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Bootstrap timings total 0.003710 (create 0.000113, send 0.000246, recv 0.001250, ring 0.000248, delay 0.000000)\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m INFO 10-15 15:48:58 [monitor.py:34] torch.compile takes 4.10 s in total\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO comm 0x55c68e0769f0 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Channel 00/02 : 0 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Channel 01/02 : 0 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2257 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2258 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO ncclCommInitRankConfig comm 0x55c68e0769f0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0xbe71890bb1555e6c - Init COMPLETE\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2256 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2758 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2757 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2757 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2757 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m statefulset-0:2629:2757 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/Socket/0\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m statefulset-1:2096:2259 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m 2025-10-15 15:51:13,940\tINFO torch_tensor_accelerator_channel.py:807 -- Creating communicator group 1c6c89ce-a9b4-445e-b941-c242dfe5b9a4 on actors: [Actor(RayWorkerWrapper, 053107a61e8572d095469c9308000000), Actor(RayWorkerWrapper, 9c0fdac4fa1267e47aa0412508000000)]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]gineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%| | 17/19 [00:00<00:00, 21.77it/s]\u001b[32m [repeated 11x across cluster]\u001b[0mper pid=2096)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|| 19/19 [00:00<00:00, 20.97it/s]DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n",
      "Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n",
      "Capturing CUDA graphs (decode, FULL):  82%| | 9/11 [00:00<00:00, 22.51it/s]\u001b[32m [repeated 5x across cluster]\u001b[0mm \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n",
      "Capturing CUDA graphs (decode, FULL): 100%|| 11/11 [00:00<00:00, 22.34it/s]\u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=2096)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m /usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/ray_communicator.py:107: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)\n",
      "\u001b[36m(RayWorkerWrapper pid=2096, ip=10.244.15.139)\u001b[0m   actor_id_tensor = torch.frombuffer(\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m 2025-10-15 15:51:14,018\tINFO torch_tensor_accelerator_channel.py:833 -- Communicator group initialized.\n",
      "\u001b[33m(raylet, ip=10.244.15.139)\u001b[0m [2025-10-15 15:51:17,651 I 2254 2254] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m INFO 2025-10-15 15:51:38,522 default_LLMServer:Qwen--Qwen3-14B kehryfqx 02468d42-12d2-4d15-a6aa-886fa13abe5d -- CALL /v1/chat/completions OK 25990.7ms\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m /usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/ray_communicator.py:107: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2629, ip=10.244.16.11)\u001b[0m   actor_id_tensor = torch.frombuffer(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:Qwen--Qwen3-14B pid=1919, ip=10.244.15.139)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2047)\u001b[0;0m \u001b[33m(raylet)\u001b[0m [2025-10-15 15:51:17,651 I 2254 2254] logging.cc:303: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n",
      "\u001b[36m(ServeReplica:default:LLMRouter pid=1588, ip=10.244.16.11)\u001b[0m INFO 2025-10-15 15:51:38,525 default_LLMRouter irmvcl1z 02468d42-12d2-4d15-a6aa-886fa13abe5d -- POST /v1/chat/completions 200 26001.9ms\n"
     ]
    }
   ],
   "source": [
    "ray.init(address=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c72fbed-614b-4cf0-af84-2fc443544b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-10-15 15:46:26,708 serve 4209 -- Deleting app ['default']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    serve.delete('default')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c81485-5766-4b2e-8121-a1a629184a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
